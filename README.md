## Hi there ðŸ‘‹

<!--
**DCBEV/DCBEV** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
This repositary contains the official Pytorch implementation for paper BEV-DAR: Enhancing Monocular Bird's Eye View Segmentation with Depth-Aware Rasterization
![image]([https://user-images.githubusercontent.com/77472466/162715638-145897ba-2c35-4734-b6a7-b30048ab80f8.png](https://github.com/BEV-DAR/BEV-DAR/blob/main/compare.png))

## Install
To use our code, please install the following dependencies:
* torch==1.9.1
* torchvison==0.10.1
* mmcv-full==1.3.15
* CUDA 9.2+

## Data Preparation
We conduct experiments of [nuScenes](https://www.nuscenes.org/download), [Argoverse](https://www.argoverse.org/),
Please down the datasets and place them under /data/nuscenes/ and so on. Note that *calib.json* contains the intrinsics and extrinsics matrixes of every image. 
Refer to the script *[make_labels](https://github.com/tom-roddick/mono-semantic-maps/blob/master/scripts)* to get the BEV annotation for nuScenes and Argoverse, respectively. The datasets' structures look like: 
### Dataset Structure
```
data
â”œâ”€â”€ nuscenes
|   â”œâ”€â”€ img_dir
|   â”œâ”€â”€ ann_bev_dir
|   â”œâ”€â”€ calib.json
â”œâ”€â”€ argoversev1.0
|   â”œâ”€â”€ img_dir
|   â”œâ”€â”€ ann_bev_dir
|   â”œâ”€â”€ calib.json
â”œâ”€â”€ kitti_processed
|   â”œâ”€â”€ kitti_raw
|   |   â”œâ”€â”€ img_dir
|   |   â”œâ”€â”€ ann_bev_dir
|   |   â”œâ”€â”€ calib.json
|   â”œâ”€â”€ kitti_odometry
|   |   â”œâ”€â”€ img_dir
|   |   â”œâ”€â”€ ann_bev_dir
|   |   â”œâ”€â”€ calib.json
|   â”œâ”€â”€ kitti_object
|   |   â”œâ”€â”€ img_dir
|   |   â”œâ”€â”€ ann_bev_dir
|   |   â”œâ”€â”€ calib.json

### Prepare calib.json
"calib.json" contains the camera parameters of each image. Readers can generate the "calib.json" file by the instruction of [nuScenes](https://www.nuscenes.org/nuscenes#download), [Argoverse](https://www.argoverse.org/), [Kitti Raw](http://www.cvlibs.net/datasets/kitti/raw_data.php), [Kitti Odometry](http://www.cvlibs.net/datasets/kitti/eval_odometry.php), and [Kitti 3D Object](http://www.cvlibs.net/datasets/kitti/eval_3dobject.php). We also upload *calib.json* for each dataset to [google drive](https://drive.google.com/drive/folders/1Ahaed1OsA1EqlJOCHHN-MQQr2VpF8H7U?usp=sharing) and [Baidu Net Disk](https://pan.baidu.com/s/1wEzHWkazS5vLPZJVjpzHMw?pwd=2022).

## Training
Take nuScenes as an example. To train a semantic segmentation model under a specific configuration, run:
```
cd DAR
```
python -m torch.distributed.launch --nproc_per_node 1 --master_port 14300 tools/train.py ./configs/DAR/dcbev.py.py --work-dir ./models_dir/dcbev.py --launcher pytorch
```
python -m torch.distributed.launch --nproc_per_node 1 --master_port 14300 tools/train.py /home/Xyx/MDT-BEV/configs/DAR/Journal.py --work-dir ./models_dir/deRe50 --launcher pytorch

python -m torch.distributed.launch --nproc_per_node 1 --master_port 14300 tools/test.py /home/Xyx/MDT-BEV/configs/DAR/Journal.py /home/Xyx/MDT-BEV/models_dir/deRe50/iter_840000.pth --out ./results/DAR_nuscenes/DAR_nuscenes.pkl --eval mIoU --launcher pytorch
## Evaluation
To evaluate the performance, run the following command:
```
cd DAR
```
python -m torch.distributed.launch --nproc_per_node ${NUM_GPU} --master_port ${PORT} tools/test.py ${CONFIG} ${MODEL_PATH} --out ${SAVE_RESULT_PATH} --eval ${METRIC} --launcher pytorch
```
The weight for nuscenes is available at https://pan.baidu.com/s/1v6yox2KypJ6Sx9bd53IM1Q?pwd=h1g4 

For example, we evaluate the mIoU on nuScenes by running:
```
cd DAR
```
CUDA_VISIBLE_DEVICE=3 python -m torch.distributed.launch --nproc_per_node 1 --master_port 14300 tools/test.py /home/Xyx/MDT-BEV/configs/DAR/Journal.py ./models_dir/deRe30/iter_840000.pth --out ./results/DAR_nuscenes/DAR_nuscenes.pkl --eval mIoU --launcher pytorch
```
CUDA_VISIBLE_DEVICE=3 python -m torch.distributed.launch --nproc_per_node 1 --master_port 14300 tools/test.py /home/Xyx/MDT-BEV/configs/DAR/Journal.py ./models_dir/deRe50/iter_840000.pth --out ./results/DAR_nuscenes/DAR_nuscenes.pkl --eval mIoU --launcher pytorch
## Visulization
To get the visulization results of the model, we first change the output_type from 'iou' to 'seg' in the testing process.
```
    # change the output_type from 'iou' to 'seg'
    test_cfg=dict(mode='whole',output_type='seg',positive_thred=0.5)
)
```
And then, we can generate the visualization results by running the following command:
```
python -m torch.distributed.launch --nproc_per_node 4 --master_port 14300 tools/test.py /home/Xyx/MDT-BEV/configs/DAR/Journal.py /home/Xyx/MDT-BEV/iter_660000.pth --format-only --eval-options "imgfile_prefix=./models_dir/" --launcher pytorch
```
## Acknowledgement
Our work is partially based on [mmseg](https://github.com/open-mmlab/mmsegmentation) and [HFT](https://github.com/JiayuZou2020/HFT/tree/main). Thanks for their contributions to the research community.

